{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40ebadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17e3266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = pd.read_csv(\"data.csv\", index_col='id')\n",
    "data.columns =  [re.sub(' \\[.*\\]', '', c) for c in data.columns]\n",
    "\n",
    "\n",
    "data_ext = pd.read_csv(\"original.csv\", index_col='id')\n",
    "data_ext.columns = [re.sub(' \\[.*\\]', '', c) for c in data_ext.columns]\n",
    "\n",
    "\n",
    "target_col = 'x_e_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bceabee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is from Mykhailo Savchenko @shalfey https://www.kaggle.com/competitions/playground-series-s3e15/discussion/413826\n",
    "\n",
    "author_geometry_mapping = { 'Inasaka': 'tube', 'Peskov': 'tube', 'Thompson': 'tube', 'Weatherhead': 'tube', 'Williams': 'tube', 'Beus': 'annulus', 'Janssen': 'annulus', 'Mortimore': 'annulus', 'Kossolapov': 'plate', 'Richenderfer': 'plate' }\n",
    "data['geometry'] = data['geometry'].fillna(data['author'].map(author_geometry_mapping))\n",
    "\n",
    "data.loc[(data['D_e'].isna() | data['D_h'].isna()) & (data['geometry'].isna()), 'geometry'] = 'tube'\n",
    "data['D_e'] = data.apply(lambda row: row['D_h'] if row['geometry'] == 'tube' and pd.isna(row['D_e']) else row['D_e'], axis=1)\n",
    "data['D_h'] = data.apply(lambda row: row['D_e'] if row['geometry'] == 'tube' and pd.isna(row['D_h']) else row['D_h'], axis=1)\n",
    "data.loc[data['D_h'] == data['D_e'], 'geometry'] = 'tube'\n",
    "\n",
    "author_list = ['Inasaka', 'Peskov', 'Thompson', 'Weatherhead', 'Williams']\n",
    "filtered_rows = data[(data['author'].isin(author_list)) & (data['geometry'] != 'tube')]\n",
    "data.loc[filtered_rows.index, 'geometry'] = 'tube'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "914274ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Pre-processing\n",
    "\n",
    "#Inasaka\n",
    "data.loc[(data['author'] == 'Inasaka') & ((data['D_h'] == 3.0) | (data['length'] == 100.0)) & (data['D_e'].isnull()), 'D_e'] = 3\n",
    "data.loc[(data['author'] == 'Inasaka') & ((data['D_e'] == 3.0) | (data['length'] == 100.0)) & (data['D_h'].isnull()), 'D_h'] = 3\n",
    "data.loc[(data['author'] == 'Inasaka') & ((data['D_e'] == 3.0) | (data['D_h'] == 3.0)) & (data['length'].isnull()), 'length'] = 100\n",
    "\n",
    "#Kossolapov\n",
    "data.loc[(data['author'] == 'Kossolapov') & ((data['D_h'] == 120.0) | (data['length'] == 10.0)) & (data['D_e'].isnull()), 'D_e'] = 15\n",
    "data.loc[(data['author'] == 'Kossolapov') & ((data['D_e'] == 15.0) | (data['length'] == 10.0)) & (data['D_h'].isnull()), 'D_h'] = 120\n",
    "data.loc[(data['author'] == 'Kossolapov') & ((data['D_e'] == 15.0) | (data['D_h'] == 120.0)) & (data['length'].isnull()), 'length'] = 10\n",
    "data.loc[(data['author'] == 'Kossolapov') & (data['pressure'].isnull()), 'pressure'] = 0.1\n",
    "\n",
    "#Mortimore\n",
    "data.loc[(data['author'] == 'Mortimore') & ((data['D_h'] == 13.3) | (data['length'] == 2134.0)) & (data['D_e'].isnull()), 'D_e'] = 5\n",
    "data.loc[(data['author'] == 'Mortimore') & ((data['D_e'] == 5.0) | (data['length'] == 2134.0)) & (data['D_h'].isnull()), 'D_h'] = 13.3\n",
    "data.loc[(data['author'] == 'Mortimore') & ((data['D_e'] == 5.0) | (data['D_h'] == 13.3)) & (data['length'].isnull()), 'length'] = 2134\n",
    "\n",
    "#Richenderfer\n",
    "data.loc[(data['author'] == 'Richenderfer') & ((data['D_h'] == 120.0) | (data['length'] == 10.0)) & (data['D_e'].isnull()), 'D_e'] = 15\n",
    "data.loc[(data['author'] == 'Richenderfer') & ((data['D_e'] == 15.0) | (data['length'] == 10.0)) & (data['D_h'].isnull()), 'D_h'] = 120\n",
    "data.loc[(data['author'] == 'Richenderfer') & ((data['D_e'] == 15.0) | (data['D_h'] == 120.0)) & (data['length'].isnull()), 'length'] = 10\n",
    "\n",
    "#Peskov\n",
    "data.loc[(data['author'] == 'Peskov') & ((data['D_h'] == 10.0) | (data['geometry'] == 'tube')) & (data['D_e'].isnull()), 'D_e'] = 10\n",
    "data.loc[(data['author'] == 'Peskov') & ((data['D_e'] == 10.0) | (data['geometry'] == 'tube')) & (data['D_h'].isnull()), 'D_h'] = 10\n",
    "data.loc[(data['author'] == 'Peskov') & ((data['D_e'] == 10.0) | (data['D_h'] == 10.0)) & (data['geometry'].isnull()), 'geometry'] = 'tube'\n",
    "\n",
    "#Williams\n",
    "data.loc[(data['author'] == 'Williams') & ((data['D_h'] == 9.5) | (data['length'] == 1836)) & (data['D_e'].isnull()), 'D_e'] = 9.5\n",
    "data.loc[(data['author'] == 'Williams') & ((data['D_e'] == 9.5) | (data['length'] == 1836)) & (data['D_h'].isnull()), 'D_h'] = 9.5\n",
    "data.loc[(data['author'] == 'Williams') & ((data['D_e'] == 9.5) | (data['D_h'] == 9.5)) & (data['length'].isnull()), 'length'] = 1836\n",
    "data.loc[(data['author'] == 'Williams') & (data['geometry'].isnull()), 'geometry'] = 'tube'\n",
    "\n",
    "#Weatherhead\n",
    "data.loc[(data['author'] == 'Weatherhead') & (data['D_h'] == 7.7) & (data['D_e'].isnull()), 'D_e'] = 7.7\n",
    "data.loc[(data['author'] == 'Weatherhead') & (data['D_h'] == 11.1) & (data['D_e'].isnull()), 'D_e'] = 11.1\n",
    "data.loc[(data['author'] == 'Weatherhead') & (data['D_e'] == 7.7) & (data['D_h'].isnull()), 'D_h'] = 7.7\n",
    "data.loc[(data['author'] == 'Weatherhead') & (data['D_e'] == 11.1) & (data['D_h'].isnull()), 'D_h'] = 11.1\n",
    "data.loc[(data['author'] == 'Weatherhead') & (data['geometry'].isnull()), 'geometry'] = 'tube'\n",
    "data.loc[(data['author'] == 'Weatherhead') & ((data['D_h'] == 7.7) | (data['D_h'] == 11.1)) & (data['length'].isnull()), 'length'] = 457\n",
    "data.loc[(data['author'] == 'Weatherhead') & (data['pressure'].isnull()), 'pressure'] = 13.79\n",
    "\n",
    "#Beus\n",
    "data.loc[(data['author'] == 'Beus') & ((data['D_h'] == 15.2) | (data['length'] == 2134.0)) & (data['D_e'].isnull()), 'D_e'] = 5.6\n",
    "data.loc[(data['author'] == 'Beus') & ((data['D_e'] == 5.6) | (data['length'] == 2134.0)) & (data['D_h'].isnull()), 'D_h'] = 15.2\n",
    "data.loc[(data['author'] == 'Beus') & ((data['D_e'] == 5.6) | (data['D_h'] == 15.2)) & (data['length'].isnull()), 'length'] = 2134\n",
    "data.loc[(data['author'] == 'Beus') & (data['geometry'].isnull()), 'geometry'] = 'annulus'\n",
    "\n",
    "#Janssen\n",
    "data.loc[(data['author'] == 'Janssen') & (data['D_e'] == 12.7) & (data['length'] < 1200.0) & (data['D_h'].isnull()), 'D_h'] = 38.1\n",
    "data.loc[(data['author'] == 'Janssen') & (data['D_e'] == 12.7) & (data['length'] > 1200.0) & (data['D_h'].isnull()), 'D_h'] = 42.3\n",
    "data.loc[(data['author'] == 'Janssen') & ((data['D_h'] == 38.1) | (data['D_h'] == 42.3)) & (data['D_e'].isnull()), 'D_e'] = 12.7\n",
    "\n",
    "data.loc[(data['author'] == 'Janssen') & ((data['D_h'] == 15.9) | (data['length'] == 914)) & (data['D_e'].isnull()), 'D_e'] = 6.4\n",
    "data.loc[(data['author'] == 'Janssen') & ((data['D_e'] == 6.4) | (data['length'] == 914)) & (data['D_h'].isnull()), 'D_h'] = 15.9\n",
    "data.loc[(data['author'] == 'Janssen') & ((data['D_e'] == 6.4) | (data['D_h'] == 15.9)) & (data['length'].isnull()), 'length'] = 914\n",
    "\n",
    "data.loc[(data['author'] == 'Janssen') & (data['D_h'] == 96.3) & (data['length'] == 1778) & (data['D_e'].isnull()), 'D_e'] = 22.2\n",
    "data.loc[(data['author'] == 'Janssen') & (data['D_e'] == 22.2) & (data['length'] == 1778) & (data['D_h'].isnull()), 'D_h'] = 96.3\n",
    "data.loc[(data['author'] == 'Janssen') & (data['D_e'] == 22.2) & (data['D_h'] == 96.3) & (data['length'].isnull()), 'length'] = 1778\n",
    "\n",
    "data.loc[(data['author'] == 'Janssen') & (data['D_h'] == 24.6) & (data['D_e'].isnull()), 'D_e'] = 8.5\n",
    "data.loc[(data['author'] == 'Janssen') & (data['D_e'] == 8.5) & (data['length'] == 1778) & (data['D_h'].isnull()), 'D_h'] = 24.6\n",
    "data.loc[(data['author'] == 'Janssen') & (data['D_e'] == 8.5) & (data['D_h'] == 24.6) & (data['length'].isnull()), 'length'] = 1778\n",
    "\n",
    "data.loc[(data['author'] == 'Janssen') & (data['geometry'].isnull()), 'geometry'] = 'annulus'\n",
    "\n",
    "#Thompson\n",
    "data.loc[(data['author'] == 'Thompson') & (data['D_h'].notnull()) & (data['D_e'].isnull()), 'D_e'] = data.loc[(data['author'] == 'Thompson') & (data['D_h'].notnull()) & (data['D_e'].isnull()), 'D_h']\n",
    "data.loc[(data['author'] == 'Thompson') & (data['D_e'].notnull()) & (data['D_h'].isnull()), 'D_h'] = data.loc[(data['author'] == 'Thompson') & (data['D_e'].notnull()) & (data['D_h'].isnull()), 'D_e']\n",
    "\n",
    "data.loc[(data['author'] == 'Thompson') & (data['geometry'].isnull()), 'geometry'] = 'tube'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e32930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save original target NaN values, add original dataset to aid imputing, grab number of rows of synthetic data\n",
    "\n",
    "target = data[target_col]\n",
    "data_all = pd.concat([data, data_ext], ignore_index=True)\n",
    "\n",
    "data_nrows = data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "905a9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use K-nearest neighbors for imputation of remaining numerical values\n",
    "\n",
    "knni = KNNImputer(n_neighbors = 89)\n",
    "data_imp = knni.fit_transform(data_all.select_dtypes(include=np.number))\n",
    "data_imp = pd.DataFrame(data_imp, columns=data_all.select_dtypes(include=np.number).columns)\n",
    "data_all = pd.concat([data_all.iloc[:,:2], data_imp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bb9be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove original dataset from data and restore NaN target values\n",
    "\n",
    "data = data_all.iloc[:data_nrows]\n",
    "data.loc[:,target_col] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8de1ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any remaining NaN values from categorical columns\n",
    "\n",
    "#Inasaka\n",
    "data.loc[(data['D_h'] == 3.0) & (data['D_e'] == 3.0) & (data['length'] == 100.0) & (data['author'].isnull()), 'author'] = 'Inasaka'\n",
    "data.loc[(data['author'] == 'Inasaka') & (data['geometry'].isnull()), 'geometry'] = 'tube'\n",
    "\n",
    "#Kossolapov (too similar to Richenderfer, and less common)\n",
    "#data.loc[(data['D_h'] == 120.0) & (data['D_e'] == 15.0) & (data['length'] == 10.0) & (data['author'].isnull()), 'author'] = 'Kossolapov'\n",
    "data.loc[(data['author'] == 'Kossolapov') & (data['geometry'].isnull()), 'geometry'] = 'plate'\n",
    "\n",
    "#Mortimore\n",
    "data.loc[(data['D_h'] == 13.3) & (data['D_e'] == 5.0) & (data['length'] == 2134.0) & (data['author'].isnull()), 'author'] = 'Mortimore'\n",
    "data.loc[(data['author'] == 'Mortimore') & (data['geometry'].isnull()), 'geometry'] = 'annulus'\n",
    "\n",
    "#Richenderfer\n",
    "data.loc[(data['D_h'] == 120.0) & (data['D_e'] == 15.0) & (data['length'] == 10.0) & (data['author'].isnull()), 'author'] = 'Richenderfer'\n",
    "data.loc[(data['author'] == 'Richenderfer') & (data['geometry'].isnull()), 'geometry'] = 'plate'\n",
    "\n",
    "#Peskov (This overlaps a bit with Thompson, but a D_e and D_h of 10,10 is more commonly Peskov)\n",
    "data.loc[(data['D_h'] == 10.0) & (data['D_e'] == 10.0) & (data['author'].isnull()), 'author'] = 'Peskov'\n",
    "data.loc[(data['author'] == 'Peskov') & (data['geometry'].isnull()), 'geometry'] = 'tube'\n",
    "\n",
    "#Williams (This overlaps a bit with Thompson, but a D_e and D_h of 9.5,9.5 is more commonly Williams)\n",
    "data.loc[(data['D_h'] == 9.5) & (data['D_e'] == 9.5) & (data['length'] == 1836) & (data['author'].isnull()), 'author'] = 'Williams'\n",
    "data.loc[(data['author'] == 'Williams') & (data['geometry'].isnull()), 'geometry'] = 'tube'\n",
    "\n",
    "#Weatherhead (This overlaps a bit with Thompson, but a D_e and D_h of 7.7,7.7 is more commonly Weatherhead)\n",
    "data.loc[(data['D_h'] == 7.7) & (data['D_e'] == 7.7) & (data['length'] == 457) & (data['author'].isnull()), 'author'] = 'Weatherhead'\n",
    "data.loc[(data['D_h'] == 11.1) & (data['D_e'] == 11.1) & (data['length'] == 457) & (data['author'].isnull()), 'author'] = 'Weatherhead'\n",
    "data.loc[(data['author'] == 'Weatherhead') & (data['geometry'].isnull()), 'geometry'] = 'tube'\n",
    "\n",
    "#Beus\n",
    "data.loc[(data['D_h'] == 15.2) & (data['D_e'] == 5.6) & (data['length'] == 2134.0) & (data['author'].isnull()), 'author'] = 'Beus'\n",
    "data.loc[(data['author'] == 'Beus') & (data['geometry'].isnull()), 'geometry'] = 'annulus'\n",
    "\n",
    "#Janssen\n",
    "data.loc[((data['D_h'] == 38.1) | (data['D_h'] == 42.3)) & (data['D_e'] == 12.7) & (data['author'].isnull()), 'author'] = 'Janssen'\n",
    "\n",
    "data.loc[(data['D_h'] == 15.9) & (data['D_e'] == 6.4) & (data['length'] == 914.0) & (data['author'].isnull()), 'author'] = 'Janssen'\n",
    "\n",
    "data.loc[(data['D_h'] == 96.3) & (data['D_e'] == 22.2) & (data['length'] == 1778.0) & (data['author'].isnull()), 'author'] = 'Janssen'\n",
    "\n",
    "data.loc[(data['D_h'] == 24.6) & (data['D_e'] == 8.5) & (data['length'] == 1778.0) & (data['author'].isnull()), 'author'] = 'Janssen'\n",
    "\n",
    "data.loc[(data['author'] == 'Janssen') & (data['geometry'].isnull()), 'geometry'] = 'annulus'\n",
    "\n",
    "#Thompson\n",
    "data.loc[data['author'].isnull(), 'author'] = 'Thompson'\n",
    "data.loc[data['geometry'].isnull(), 'geometry'] = 'tube'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dc12fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurecreation(data):\n",
    "    # Feature Creation\n",
    "    # Adiabatic surface area\n",
    "    data.loc[:,'adiabatic_surface_area'] = data.loc[:,'D_e'] * data.loc[:,'length']\n",
    "\n",
    "    \n",
    "    # Surface area to horizontal diameter ratio\n",
    "    data.loc[:,'surface_diameter_ratio'] = data.loc[:,'D_e'] / data.loc[:,'D_h']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1e98c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create additional features\n",
    "\n",
    "data = featurecreation(data)\n",
    "data_ext = featurecreation(data_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bca58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test sets\n",
    "\n",
    "data_tr = data[data[target_col].notnull()]\n",
    "\n",
    "data_test = data[data[target_col].isnull()]\n",
    "\n",
    "unique_targets = data_tr[target_col].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a39262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired by aldparis @adaubas https://www.kaggle.com/competitions/playground-series-s3e14/discussion/409242\n",
    "\n",
    "class PLSRegressionWrapper(PLSRegression):\n",
    "    def transform(self, X):\n",
    "        return super().transform(X)\n",
    "\n",
    "\n",
    "class FeatureEngineering():\n",
    "\n",
    "    def __init__(self, cat_cols, num_cols, feat_group_A = ['pressure', 'mass_flux', 'chf_exp'], feat_group_B = ['pressure', 'mass_flux', 'chf_exp']):\n",
    "        \n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        \n",
    "        self.feat_group_A = feat_group_A\n",
    "        self.feat_group_B = feat_group_B\n",
    "        self.groups = [self.feat_group_A, self.feat_group_B]\n",
    "\n",
    "    def fit(self, x, y=None, n_components_A=0, n_components_B=0, method_A='pca', method_B='pca'):\n",
    "        \n",
    "        self.cat_transformer = make_pipeline(OneHotEncoder())\n",
    "        self.num_transformer = make_pipeline(StandardScaler())\n",
    "        \n",
    "        \n",
    "        self.preprocessor = ColumnTransformer(\n",
    "                            transformers=[('cat', self.cat_transformer, self.cat_cols),\n",
    "                                          ('num', self.num_transformer, self.num_cols)\n",
    "                                          ])\n",
    "        \n",
    "        self.preprocessor.fit(x)\n",
    "        \n",
    "        self.n_components = [n_components_A, n_components_B]\n",
    "        self.methods = [method_A, method_B]\n",
    "        self.dim_red = [0]*len(self.n_components)\n",
    "        \n",
    "        for i in range(len(self.n_components)):\n",
    "            \n",
    "            if self.n_components[i] > 0:\n",
    "                \n",
    "                if self.methods[i] == 'pca':\n",
    "                    self.dim_red[i] = PCA(n_components = self.n_components[i], random_state=8)\n",
    "                    self.dim_red[i].fit(x[self.groups[i]])\n",
    "                \n",
    "                if self.methods[i] == 'umap':\n",
    "                    self.dim_red[i] = UMAP(n_components = self.n_components[i], random_state=8)\n",
    "                    self.dim_red[i].fit(x[self.groups[i]])\n",
    "                    \n",
    "                if self.methods[i] == 'pls':\n",
    "                    self.dim_red[i] = PLSRegressionWrapper(n_components = self.n_components[i])\n",
    "                    self.dim_red[i].fit(x[self.groups[i]], y)       \n",
    "                    \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "\n",
    "        df = x.copy()\n",
    "        df = self.preprocessor.transform(df)\n",
    "        \n",
    "        cols = []\n",
    "        \n",
    "        for cat in self.cat_cols:\n",
    "            cols = cols + sorted(x[cat].unique().tolist())\n",
    "        \n",
    "        cols = cols + self.num_cols\n",
    "        \n",
    "        df = pd.DataFrame(df, columns=cols)\n",
    "        \n",
    "        for i in range(len(self.n_components)):\n",
    "            \n",
    "            if self.n_components[i] > 0:\n",
    "                dr_cols = [f\"{self.methods[i]}{i}_{j}\" for j in range(self.n_components[i])]\n",
    "                df[dr_cols] = self.dim_red[i].transform(df[self.groups[i]])\n",
    "                \n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31967a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(X_tr, y_tr, X_ext, y_ext, X_test):\n",
    "    \"\"\"\n",
    "    predict_test uses KFold cross validation to return predictions\n",
    "    y_pred for test set X_test by utilizing training data X_tr,y_tr \n",
    "    and external data X_ext,y_ext\n",
    "    \n",
    "    :X_tr:   Training data features\n",
    "    :y_tr:   Training data target\n",
    "    :X_ext:  External data features added to every fold during KFold cross validation\n",
    "    :y_ext:  External data target added to every fold during KFold cross validation\n",
    "    :X_test: Test dataset used for predictions\n",
    "    \n",
    "    :return y_pred: Predictions based on test dataset X_test\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    kf = KFold(n_splits=10, random_state=8, shuffle=True)\n",
    "    y_pred = pd.Series(0, index=X_test.index)\n",
    "    \n",
    "    callbacks = [lgbm.log_evaluation(period=2001, show_stdv=False),\n",
    "                 lgbm.early_stopping(stopping_rounds=100, verbose=False)]\n",
    "    \n",
    "    \n",
    "    cat_cols_1 = ['author']\n",
    "    num_cols_1 = ['pressure', 'mass_flux', 'D_e', 'D_h','length', 'chf_exp']\n",
    "    features_1 = cat_cols_1 + num_cols_1\n",
    "    \n",
    "    cat_cols_2 = ['author']\n",
    "    num_cols_2 = ['pressure', 'mass_flux', 'D_e', 'D_h','length', 'chf_exp', 'adiabatic_surface_area', 'surface_diameter_ratio']\n",
    "    features_2 = cat_cols_2 + num_cols_2\n",
    "    \n",
    "    cat_cols_3 = ['author']\n",
    "    num_cols_3 = ['pressure', 'mass_flux', 'D_e', 'D_h','length', 'chf_exp', 'adiabatic_surface_area', 'surface_diameter_ratio']\n",
    "    features_3 = cat_cols_3 + num_cols_3\n",
    "    \n",
    "    cat_cols_4 = ['author']\n",
    "    num_cols_4 = ['pressure', 'mass_flux', 'D_e', 'D_h','length', 'chf_exp']\n",
    "    features_4 = cat_cols_4 + num_cols_4\n",
    "    \n",
    "    params_1 = {'objective' : 'regression_l2',\n",
    "          'metric' : 'rmse',\n",
    "          'n_estimators' : 2000,\n",
    "          'learning_rate': 0.01016320397965528,\n",
    "          'max_depth': 10,\n",
    "          'num_leaves': 1239,\n",
    "          'colsample_bytree': 0.342285821102964,\n",
    "          'max_bin': 588,\n",
    "          'min_child_samples': 17,\n",
    "          'subsample': 0.4967871237928858,\n",
    "          'reg_alpha': 0.011037673638980894,\n",
    "          'reg_lambda': 2.67015907440627,\n",
    "          'boosting_type': 'gbdt',\n",
    "          'data_sample_strategy': 'goss',\n",
    "          'verbose': -1} \n",
    "    \n",
    "    params_2 = {'objective' : 'regression_l2',\n",
    "          'metric' : 'rmse',\n",
    "          'n_estimators' : 2000,\n",
    "          'learning_rate': 0.014936477998424232,\n",
    "          'max_depth': 9,\n",
    "          'num_leaves': 338,\n",
    "          'colsample_bytree': 0.2834900944718662,\n",
    "          'max_bin': 939,\n",
    "          'min_child_samples': 20,\n",
    "          'subsample': 0.3882591137382151,\n",
    "          'reg_alpha': 0.006987253765394774,\n",
    "          'reg_lambda': 1.9803759175177449,\n",
    "          'boosting_type': 'gbdt',\n",
    "          'data_sample_strategy': 'goss',\n",
    "          'verbose': -1}\n",
    "    \n",
    "    params_3 = {'objective' : 'regression_l2',\n",
    "          'metric' : 'rmse',\n",
    "          'n_estimators' : 2000,\n",
    "          'learning_rate': 0.030194630334037888,\n",
    "          'max_depth': 10,\n",
    "          'num_leaves': 1456,\n",
    "          'colsample_bytree': 0.3677827029249661,\n",
    "          'max_bin': 910,\n",
    "          'min_child_samples': 16,\n",
    "          'subsample': 0.5430561217554016,\n",
    "          'reg_alpha': 0.05766238378926456,\n",
    "          'reg_lambda': 3.827008003976793,\n",
    "          'boosting_type': 'gbdt',\n",
    "          'verbose': -1}\n",
    "    \n",
    "    \n",
    "    params_4 = {'objective' : 'reg:squarederror',\n",
    "          'eval_metric' : 'rmse',\n",
    "          'verbosity' : 0,\n",
    "          'silence' : True,\n",
    "          'n_estimators' : 2000,\n",
    "          'learning_rate': 0.05957325414211462,\n",
    "          'max_depth': 8,\n",
    "          'colsample_bytree': 0.3892362281817108,\n",
    "          'alpha': 0.3060792413899057,\n",
    "          'lambda': 2.2730782167388046,\n",
    "          'gamma': 0.007808066644093886,\n",
    "          'max_bin': 526,\n",
    "          'subsample': 0.7846145456232383,\n",
    "          'booster': 'gbtree',\n",
    "          'min_child_weight': 15}\n",
    "    \n",
    "    fe_1 = FeatureEngineering(cat_cols_1, num_cols_1)\n",
    "    fe_2 = FeatureEngineering(cat_cols_2, num_cols_2)\n",
    "    fe_3 = FeatureEngineering(cat_cols_3, num_cols_3)\n",
    "    fe_4 = FeatureEngineering(cat_cols_4, num_cols_4)\n",
    "    \n",
    "    \n",
    "    for i, [train_idx, val_idx] in enumerate(kf.split(X_tr, y_tr)):\n",
    "        X_t, X_val = X_tr.iloc[train_idx], X_tr.iloc[val_idx]\n",
    "        y_t, y_val = y_tr.iloc[train_idx], y_tr.iloc[val_idx]\n",
    "        \n",
    "        X_train = pd.concat([X_t, X_ext], ignore_index=True)\n",
    "        y_train = pd.concat([y_t, y_ext], ignore_index=True)\n",
    "        \n",
    "        fe_1.fit(X_train, y_train, 0, 0)\n",
    "        fe_2.fit(X_train, y_train, 0, 0)\n",
    "        fe_3.fit(X_train, y_train, 1, 0, 'pls')      \n",
    "        fe_4.fit(X_train, y_train, 0, 0)\n",
    "        \n",
    "        X_train_transform_1 = fe_1.transform(X_train[features_1])\n",
    "        X_train_transform_2 = fe_2.transform(X_train[features_2])\n",
    "        X_train_transform_3 = fe_3.transform(X_train[features_3])\n",
    "        X_train_transform_4 = fe_4.transform(X_train[features_4])\n",
    "        \n",
    "        X_val_transform_1 = fe_1.transform(X_val[features_1])\n",
    "        X_val_transform_2 = fe_2.transform(X_val[features_2])\n",
    "        X_val_transform_3 = fe_3.transform(X_val[features_3])\n",
    "        X_val_transform_4 = fe_4.transform(X_val[features_4])\n",
    "        \n",
    "        X_test_transform_1 = fe_1.transform(X_test[features_1])\n",
    "        X_test_transform_2 = fe_2.transform(X_test[features_2])\n",
    "        X_test_transform_3 = fe_3.transform(X_test[features_3])\n",
    "        X_test_transform_4 = fe_4.transform(X_test[features_4])\n",
    "        \n",
    "        model_1 = LGBMRegressor(**params_1)\n",
    "        model_2 = LGBMRegressor(**params_2)\n",
    "        model_3 = LGBMRegressor(**params_3)\n",
    "        model_4 = XGBRegressor(**params_4)\n",
    "        \n",
    "        model_1.fit(X_train_transform_1, y_train, eval_set=[(X_val_transform_1, y_val)], callbacks=callbacks)\n",
    "        model_2.fit(X_train_transform_2, y_train, eval_set=[(X_val_transform_2, y_val)], callbacks=callbacks)\n",
    "        model_3.fit(X_train_transform_3, y_train, eval_set=[(X_val_transform_3, y_val)], callbacks=callbacks)\n",
    "        model_4.fit(X_train_transform_4, y_train, eval_set=[(X_val_transform_4, y_val)], early_stopping_rounds=100, verbose=0)\n",
    "        \n",
    "        \n",
    "        y_pred += (0.25*model_1.predict(X_test_transform_1) + 0.25*model_2.predict(X_test_transform_2) + 0.1*model_3.predict(X_test_transform_3) + 0.4*model_4.predict(X_test_transform_4))/10\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20967565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_e_out [-]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x_e_out [-]\n",
       "id             \n",
       "4           0.0\n",
       "7           0.0\n",
       "10          0.0\n",
       "12          0.0\n",
       "23          0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"sample_submission.csv\", index_col='id')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ca1cf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_e_out [-]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.008476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.079818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.043033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.005858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.044331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x_e_out [-]\n",
       "id             \n",
       "4     -0.008476\n",
       "7     -0.079818\n",
       "10    -0.043033\n",
       "12     0.005858\n",
       "23     0.044331"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr = data_tr.drop(target_col, axis=1)\n",
    "y_tr = data_tr[target_col]\n",
    "\n",
    "X_ext = data_ext.drop(target_col, axis=1)\n",
    "y_ext = data_ext[target_col]\n",
    "\n",
    "X_test = data_test.drop(target_col, axis=1)\n",
    "\n",
    "\n",
    "y_test = predict_test(X_tr, y_tr, X_ext, y_ext, X_test)\n",
    "y_test = pd.DataFrame(y_test, index=sample.index, columns=['x_e_out [-]'])\n",
    "\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "563b9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv('predictions.csv', columns=['x_e_out [-]'], index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c676c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
